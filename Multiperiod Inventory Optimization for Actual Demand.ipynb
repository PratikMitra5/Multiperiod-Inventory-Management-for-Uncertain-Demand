{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147d7837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poisson rate for demand distribution lambda: 7\n",
      "Maximum inventory allowed in the warehouse: 16\n",
      "The parameter K: 6\n",
      "The parameter alpha: 3\n",
      "The parameter beta: 2\n",
      "The discounting factor gamma: [0.98640156]\n",
      "The parameter delta: [10.7980482]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "#Enter the sum of digits in your enrollment number as seed\n",
    "results = []\n",
    "\n",
    "def environment(seed):\n",
    "  np.random.seed(seed)\n",
    "  Lambda = random.randint(7,8)\n",
    "  M = random.randint(10,20)\n",
    "  K = random.randint(4,7)\n",
    "  alpha = random.randint(2,3)\n",
    "  beta =  random.randint(1,2) \n",
    "  gamma = np.random.uniform(0.9,0.99,1)\n",
    "  delta = np.random.uniform(8,12,1)\n",
    "  return Lambda, M, K, alpha, beta, gamma, delta\n",
    "\n",
    "seed = 24\n",
    "Lambda, M, K, alpha, beta, gamma, delta = environment(seed)\n",
    "print('Poisson rate for demand distribution lambda:', Lambda)\n",
    "print('Maximum inventory allowed in the warehouse:', M)\n",
    "print('The parameter K:', K)\n",
    "print('The parameter alpha:', alpha)\n",
    "print('The parameter beta:', beta)\n",
    "print('The discounting factor gamma:', gamma)\n",
    "print('The parameter delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f48aabf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Calculating the transition probabilities and expected returns for a given state and action.\n",
    "# def probability(demand):\n",
    "#     return ((Lambda**demand)*math.exp(-1*Lambda))/float(math.factorial(demand))\n",
    "\n",
    "# states=[i for i in range(M)]\n",
    "# actions = [i for i in range(M)]\n",
    "# exp_rewards = np.zeros((M+1,M+1))\n",
    "# trans = np.zeros((M+1,M+1))\n",
    "# for state in range(M+1):\n",
    "#    for act in range(M+1):\n",
    "#         if (act>state):\n",
    "#             trans[state,act]=0\n",
    "#         elif M>=state and state>=act and act>0:\n",
    "#             demand = state - act\n",
    "#             trans[state,act] = probability(demand)\n",
    "#         elif M>=state and act == 0:\n",
    "#             res=0\n",
    "#             for i in range(state-act):\n",
    "#                 res = res + probability(i)\n",
    "#             trans[state,act] = 1-res\n",
    "# for state in range(M+1):\n",
    "#     for act in range(M+1):\n",
    "#          if state+act>M:\n",
    "#             exp_rewards[state,act] = -50000000\n",
    "#          else:\n",
    "#             OC = 0\n",
    "#             if(act>0):\n",
    "#                 OC = K + alpha*act\n",
    "#             HC = beta*(state+act)\n",
    "#             exp_rev=0\n",
    "#             for j in range(M+1):\n",
    "#                 exp_rev = exp_rev + trans[state+act,j]*delta*(state+act-j)\n",
    "#             exp_rewards[state,act] = exp_rev - HC - OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb08837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the returns for a particular (state, action, newstate) combinations.\n",
    "rewards = np.zeros((M+1,M+1,M+1))\n",
    "for state in range(M+1):\n",
    "    for act in range(M+1):\n",
    "        for newState in range(M+1):\n",
    "            if newState>(state+act):\n",
    "                rewards[state,act,newState] = -5000000\n",
    "            else:\n",
    "                OC=0\n",
    "                if(act>0):\n",
    "                    OC = K + alpha*act\n",
    "                HC = beta*(state+act)\n",
    "                rewards[state,act,newState] = delta*(state+act-newState) - HC - OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f289fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00000000e+00, -5.00000000e+06, -5.00000000e+06, ...,\n",
       "         -5.00000000e+06, -5.00000000e+06, -5.00000000e+06],\n",
       "        [-2.01951800e-01, -1.10000000e+01, -5.00000000e+06, ...,\n",
       "         -5.00000000e+06, -5.00000000e+06, -5.00000000e+06],\n",
       "        [ 5.59609640e+00, -5.20195180e+00, -1.60000000e+01, ...,\n",
       "         -5.00000000e+06, -5.00000000e+06, -5.00000000e+06],\n",
       "        ...,\n",
       "        [ 7.51726748e+01,  6.43746266e+01,  5.35765784e+01, ...,\n",
       "         -7.60000000e+01, -5.00000000e+06, -5.00000000e+06],\n",
       "        [ 8.09707230e+01,  7.01726748e+01,  5.93746266e+01, ...,\n",
       "         -7.02019518e+01, -8.10000000e+01, -5.00000000e+06],\n",
       "        [ 8.67687712e+01,  7.59707230e+01,  6.51726748e+01, ...,\n",
       "         -6.44039036e+01, -7.52019518e+01, -8.60000000e+01]],\n",
       "\n",
       "       [[ 8.79804820e+00, -2.00000000e+00, -5.00000000e+06, ...,\n",
       "         -5.00000000e+06, -5.00000000e+06, -5.00000000e+06],\n",
       "        [ 8.59609640e+00, -2.20195180e+00, -1.30000000e+01, ...,\n",
       "         -5.00000000e+06, -5.00000000e+06, -5.00000000e+06],\n",
       "        [ 1.43941446e+01,  3.59609640e+00, -7.20195180e+00, ...,\n",
       "         -5.00000000e+06, -5.00000000e+06, -5.00000000e+06],\n",
       "        ...,\n",
       "        [ 8.39707230e+01,  7.31726748e+01,  6.23746266e+01, ...,\n",
       "         -6.72019518e+01, -7.80000000e+01, -5.00000000e+06],\n",
       "        [ 8.97687712e+01,  7.89707230e+01,  6.81726748e+01, ...,\n",
       "         -6.14039036e+01, -7.22019518e+01, -8.30000000e+01],\n",
       "        [ 9.55668194e+01,  8.47687712e+01,  7.39707230e+01, ...,\n",
       "         -5.56058554e+01, -6.64039036e+01, -7.72019518e+01]],\n",
       "\n",
       "       [[ 1.75960964e+01,  6.79804820e+00, -4.00000000e+00, ...,\n",
       "         -5.00000000e+06, -5.00000000e+06, -5.00000000e+06],\n",
       "        [ 1.73941446e+01,  6.59609640e+00, -4.20195180e+00, ...,\n",
       "         -5.00000000e+06, -5.00000000e+06, -5.00000000e+06],\n",
       "        [ 2.31921928e+01,  1.23941446e+01,  1.59609640e+00, ...,\n",
       "         -5.00000000e+06, -5.00000000e+06, -5.00000000e+06],\n",
       "        ...,\n",
       "        [ 9.27687712e+01,  8.19707230e+01,  7.11726748e+01, ...,\n",
       "         -5.84039036e+01, -6.92019518e+01, -8.00000000e+01],\n",
       "        [ 9.85668194e+01,  8.77687712e+01,  7.69707230e+01, ...,\n",
       "         -5.26058554e+01, -6.34039036e+01, -7.42019518e+01],\n",
       "        [ 1.04364868e+02,  9.35668194e+01,  8.27687712e+01, ...,\n",
       "         -4.68078072e+01, -5.76058554e+01, -6.84039036e+01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.23172675e+02,  1.12374627e+02,  1.01576578e+02, ...,\n",
       "         -2.80000000e+01, -5.00000000e+06, -5.00000000e+06],\n",
       "        [ 1.22970723e+02,  1.12172675e+02,  1.01374627e+02, ...,\n",
       "         -2.82019518e+01, -3.90000000e+01, -5.00000000e+06],\n",
       "        [ 1.28768771e+02,  1.17970723e+02,  1.07172675e+02, ...,\n",
       "         -2.24039036e+01, -3.32019518e+01, -4.40000000e+01],\n",
       "        ...,\n",
       "        [ 1.98345350e+02,  1.87547301e+02,  1.76749253e+02, ...,\n",
       "          4.71726748e+01,  3.63746266e+01,  2.55765784e+01],\n",
       "        [ 2.04143398e+02,  1.93345350e+02,  1.82547301e+02, ...,\n",
       "          5.29707230e+01,  4.21726748e+01,  3.13746266e+01],\n",
       "        [ 2.09941446e+02,  1.99143398e+02,  1.88345350e+02, ...,\n",
       "          5.87687712e+01,  4.79707230e+01,  3.71726748e+01]],\n",
       "\n",
       "       [[ 1.31970723e+02,  1.21172675e+02,  1.10374627e+02, ...,\n",
       "         -1.92019518e+01, -3.00000000e+01, -5.00000000e+06],\n",
       "        [ 1.31768771e+02,  1.20970723e+02,  1.10172675e+02, ...,\n",
       "         -1.94039036e+01, -3.02019518e+01, -4.10000000e+01],\n",
       "        [ 1.37566819e+02,  1.26768771e+02,  1.15970723e+02, ...,\n",
       "         -1.36058554e+01, -2.44039036e+01, -3.52019518e+01],\n",
       "        ...,\n",
       "        [ 2.07143398e+02,  1.96345350e+02,  1.85547301e+02, ...,\n",
       "          5.59707230e+01,  4.51726748e+01,  3.43746266e+01],\n",
       "        [ 2.12941446e+02,  2.02143398e+02,  1.91345350e+02, ...,\n",
       "          6.17687712e+01,  5.09707230e+01,  4.01726748e+01],\n",
       "        [ 2.18739494e+02,  2.07941446e+02,  1.97143398e+02, ...,\n",
       "          6.75668194e+01,  5.67687712e+01,  4.59707230e+01]],\n",
       "\n",
       "       [[ 1.40768771e+02,  1.29970723e+02,  1.19172675e+02, ...,\n",
       "         -1.04039036e+01, -2.12019518e+01, -3.20000000e+01],\n",
       "        [ 1.40566819e+02,  1.29768771e+02,  1.18970723e+02, ...,\n",
       "         -1.06058554e+01, -2.14039036e+01, -3.22019518e+01],\n",
       "        [ 1.46364868e+02,  1.35566819e+02,  1.24768771e+02, ...,\n",
       "         -4.80780720e+00, -1.56058554e+01, -2.64039036e+01],\n",
       "        ...,\n",
       "        [ 2.15941446e+02,  2.05143398e+02,  1.94345350e+02, ...,\n",
       "          6.47687712e+01,  5.39707230e+01,  4.31726748e+01],\n",
       "        [ 2.21739494e+02,  2.10941446e+02,  2.00143398e+02, ...,\n",
       "          7.05668194e+01,  5.97687712e+01,  4.89707230e+01],\n",
       "        [ 2.27537542e+02,  2.16739494e+02,  2.05941446e+02, ...,\n",
       "          7.63648676e+01,  6.55668194e+01,  5.47687712e+01]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba143e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Apply in value iteration method\n",
    "# v=np.zeros(len(states))\n",
    "# #v=v.astype('int32')\n",
    "# i=0\n",
    "# error_limit=1\n",
    "# while True:\n",
    "#   old_v=v.copy()\n",
    "#   for state in states:\n",
    "#     b=-1*sys.maxsize\n",
    "#     for action in actions:\n",
    "#       a=0\n",
    "#       for next_state in states:\n",
    "#         if(state+action>M):\n",
    "#             a=a+0\n",
    "#         else:\n",
    "#             a+=trans[state+action,next_state]*(rewards[state,action,next_state]+gamma*v[next_state])\n",
    "#       b=max(b,a)\n",
    "#     v[state]=b\n",
    "#   error=np.abs(v-old_v).sum()\n",
    "#   if i%10==0:\n",
    "#     print(f\"Iteration={i} , Error={error}\")\n",
    "\n",
    "#   if error<error_limit:\n",
    "#     break\n",
    "#   i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a5c30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Value function for each state :{v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7a3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This function return state value pair for a given value function  \n",
    "# def Q_function(v):\n",
    "#   q=np.zeros((len(states),len(actions)))\n",
    "#   for state in states:\n",
    "#     for action in actions:\n",
    "#       a=0\n",
    "#       for next_state in states:\n",
    "#         if(state+action>M):\n",
    "#             a=a+0\n",
    "#         else:  \n",
    "#             a+=trans[state+action,next_state]*(rewards[state,action,next_state]+gamma*v[next_state])\n",
    "#       q[state][action]=a\n",
    "#   return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4d91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q=Q_function(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd34db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_action=np.argmax(q,axis=1)\n",
    "# print(f\"The Optimal Policy : {best_action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c265cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand\n",
    "import pandas as pd\n",
    "dem_file = pd.read_csv(\"C:\\\\Users\\\\prati\\\\OneDrive - iitr.ac.in\\\\MBA Stuff\\\\Final Project\\\\walmart_demand_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f85e811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_demand(qos,count_dem):\n",
    "    dem_data = dem_file.iloc[9][2:]\n",
    "    dem_data = dem_data.reset_index().iloc[:,1]\n",
    "    dem_data = dem_data.loc[dem_data!=0]\n",
    "#     dem_sort = dem_data.sort_values()\n",
    "#     print(dem_sort.iloc[1100:1200])\n",
    "    count_dem=count_dem+1\n",
    "#     dem_ind = round(qos*len(dem_sort))\n",
    "#     if dem_ind == len(dem_sort):\n",
    "#         dem_ind = dem_ind-1\n",
    "#     print(qos, \" - \", count_dem)\n",
    "    return round(qos*dem_data.iloc[(count_dem-1)%len(dem_data)])\n",
    "#     return round(dem_data.iloc[(count_dem-1)%len(dem_data)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bb9b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "def RL_Environment(state,act,qos,count_dem):\n",
    "#     demand = poisson.ppf(q=random.uniform(0,1),mu=Lambda,loc=0)\n",
    "    demand = get_demand(qos,count_dem)\n",
    "#     print(demand)\n",
    "    if(state+act>M):\n",
    "        return -50000000, M+1\n",
    "    else:\n",
    "        if(demand<state+act):\n",
    "            newState = (int)(state+act-demand)\n",
    "        else:\n",
    "            newState = 0\n",
    "\n",
    "        return rewards[state,act,newState],newState,(int)(demand-(state+act))\n",
    "# print(RL_Environment(50,10))\n",
    "# # print(a)\n",
    "# # print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ac415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Optimal Policy with Quality of Service Feedback\n",
    "epsilon = 0.85\n",
    "learning_rate=0.9\n",
    "cap_miss=0\n",
    "total = 0\n",
    "miss_C=0\n",
    "q_values = np.zeros((M+1,M+1))\n",
    "qos = 1\n",
    "for j in range(2500):\n",
    "    state = 0\n",
    "    old_qv = q_values.copy()\n",
    "    for i in range(100):\n",
    "        eps_test = random.uniform(0,1)\n",
    "        if(eps_test<(1/(i+1+j*15*M))):\n",
    "            action = np.random.randint(M-state)\n",
    "        else:\n",
    "            action = np.argmax(q_values[state][0:M-state])\n",
    "\n",
    "        reward, newstate, miss = RL_Environment(state,action,qos,j*M+i)\n",
    "#         qos = (state+action) / (state+action+miss)\n",
    "        old_q = q_values[state,action]\n",
    "        td = reward + (gamma * np.max(q_values[newstate][0:M-state])) - old_q \n",
    "#         if(state==0 and newstate==0 and state+action < M-1):\n",
    "#             q_values[state,action+1] = old_q + (1/(1+i+j))*td + 1\n",
    "        if(miss>0):\n",
    "            qos = (state+action) / (state+action+miss)\n",
    "            miss_C=miss_C+1\n",
    "            act_dem = action+miss\n",
    "            if(act_dem<M-state):\n",
    "                old_q1 = q_values[state][act_dem]\n",
    "                q_values[state][act_dem] = old_q1 + (1/(1+i+j))*(td+miss*(delta-beta-alpha))\n",
    "            else:\n",
    "                if(act_dem+state>M):\n",
    "                    cap_miss = cap_miss + 1\n",
    "                q_values[state][M-state-1] = q_values[state][M-state-1] + (1/(1+i+j))*(td+(M-state-action)*(delta-beta-alpha))\n",
    "        else:\n",
    "            qos = 1\n",
    "        total=total+1\n",
    "        q_values[state,action] = old_q + (1/(1+i+j))*td\n",
    "        state = newstate\n",
    "#         if(i%10==0):\n",
    "#             print(\"td\",j,\" = \",td)\n",
    "#             print(np.abs(q_values-old_qv).sum())\n",
    "    #     if(abs(td)<0.01):\n",
    "    #         break\n",
    "    if(np.abs(q_values-old_qv).sum()<0.1):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb32f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "qos_opt_act = np.zeros((M+1))\n",
    "for i in range(M):\n",
    "    qos_opt_act[i] = np.argmax(q_values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qos_opt_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Optimal Policy without Quality of Service Feedback\n",
    "epsilon = 0.85\n",
    "learning_rate=0.9\n",
    "cap_miss=0\n",
    "total = 0\n",
    "miss_C=0\n",
    "q_values = np.zeros((M+1,M+1))\n",
    "qos = 1\n",
    "for j in range(2500):\n",
    "    state = 0\n",
    "    old_qv = q_values.copy()\n",
    "    for i in range(100):\n",
    "        eps_test = random.uniform(0,1)\n",
    "        if(eps_test<(1/(i+1+j*12*M))):\n",
    "            action = np.random.randint(M-state)\n",
    "        else:\n",
    "            action = np.argmax(q_values[state][0:M-state])\n",
    "\n",
    "        reward, newstate, miss = RL_Environment(state,action,1,j*M+i)\n",
    "#         qos = (state+action) / (state+action+miss)\n",
    "        old_q = q_values[state,action]\n",
    "        td = reward + (gamma * np.max(q_values[newstate][0:M-state])) - old_q \n",
    "#         if(state==0 and newstate==0 and state+action < M-1):\n",
    "#             q_values[state,action+1] = old_q + (1/(1+i+j))*td + 1\n",
    "        if(miss>0):\n",
    "#             qos = (state+action) / (state+action+miss)\n",
    "            miss_C=miss_C+1\n",
    "            act_dem = action+miss\n",
    "            if(act_dem<M-state):\n",
    "                old_q1 = q_values[state][act_dem]\n",
    "                q_values[state][act_dem] = old_q1 + (1/(1+i+j))*(td+miss*(delta-beta-alpha))\n",
    "            else:\n",
    "                if(act_dem+state>M):\n",
    "                    cap_miss = cap_miss + 1\n",
    "                q_values[state][M-state-1] = q_values[state][M-state-1] + (1/(1+i+j))*(td+(M-state-action)*(delta-beta-alpha))\n",
    "        else:\n",
    "            qos = 1\n",
    "        total=total+1\n",
    "        q_values[state,action] = old_q + (1/(1+i+j))*td\n",
    "        state = newstate\n",
    "        if(i%10==0):\n",
    "            print(\"td\",j,\" = \",td)\n",
    "            print(np.abs(q_values-old_qv).sum())\n",
    "    #     if(abs(td)<0.01):\n",
    "    #         break\n",
    "    if(np.abs(q_values-old_qv).sum()<0.1):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_act = np.zeros((M+1))\n",
    "for i in range(M):\n",
    "    opt_act[i] = np.argmax(q_values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e6486",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_strategies = []\n",
    "# named_strategies.append((\"Value Iteration Optimal Policy\", best_action))\n",
    "named_strategies.append((\"Q-Learning Algorithm with QoS\", qos_opt_act))\n",
    "named_strategies.append((\"Q-Learning Algorithm without QoS\", opt_act))\n",
    "safety_stock=5\n",
    "\n",
    "fill_when_0= np.zeros((M+1))\n",
    "fill_when_0[0]=M\n",
    "named_strategies.append(('Standard Policy of refilling stock when inventory is 0.', fill_when_0))\n",
    "fill_every_time= np.zeros((M+1))\n",
    "for i in range(M):\n",
    "    fill_every_time[i] = M-i\n",
    "named_strategies.append(('Standard Policy of refilling stock at every chance.', fill_every_time))\n",
    "ss_policy = np.zeros((M+1))\n",
    "for i in range(safety_stock):\n",
    "    ss_policy[i] = M-i\n",
    "named_strategies.append(('Standard Policy of refilling stock when inventory is less than {}'.format(safety_stock), ss_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f5614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_strategies(named_strategies):\n",
    "    run_cycles=2000\n",
    "    result=[]\n",
    "    for policy_name, strategy in named_strategies:\n",
    "        rewards_hist = []\n",
    "        curr_state = 0\n",
    "#         print(strategy)\n",
    "        total_reward=0\n",
    "        for j in range(run_cycles):\n",
    "            print(curr_state, \" - \",  strategy[curr_state])\n",
    "            reward, new_state, miss = RL_Environment(curr_state, (int)(strategy[curr_state]),1,j)\n",
    "            total_reward = total_reward + reward\n",
    "            rewards_hist.append(total_reward)\n",
    "            curr_state = new_state\n",
    "        series = pd.Series(rewards_hist, index=range(run_cycles), name=\"{} ({})\".format(policy_name, total_reward / run_cycles))\n",
    "        result.append(series)\n",
    "    df = pd.concat(result, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a12c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eval_strategies(named_strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c066f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(xlabel=\"Periods\",ylabel=\"Total Rewards\", figsize=(12,9),\n",
    "        title='Comparison of Policies in terms of Total Rewards vs No. of periods for real demand M={}'.format(M))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
